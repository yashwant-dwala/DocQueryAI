# DocQueryAI .... What it does?
Retrieval-Augumented Generation of Answers from the provided doc using LLM.

## TECH-STACK
ollama's llama3 model is used locally to generate answers.

Embeddings are created using sentence-transformers/all-MiniLM-L6-v2 locally.

Vector DB used is FAISS library (later can use ChromaDB)

# PRE-REQUISITES:
1. Ollama must be downloaded and running on the system.

   

LLM used are locally downloaded models in ollama (deepseek-llm:7b for code and llama3:latest for Q&A)    


